{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from flask import Flask, jsonify, request\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "# MongoDB setup\n",
    "client = MongoClient('mongodb://127.0.0.1:27017/')\n",
    "db = client['aljazeera']  # Database name\n",
    "news_collection = db['news']          # Collection for news\n",
    "courses_collection = db['courses']      # Collection for courses\n",
    "trainers_collection = db['trainers']    # Collection for trainers\n",
    "\n",
    "# Clean text helper\n",
    "def clean_text(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.news_data = []\n",
    "        \n",
    "        # Set up session headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Get BeautifulSoup object for a given URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_news_item(self, news_element):\n",
    "        \"\"\"Extract information from a news item element\"\"\"\n",
    "        try:\n",
    "            news = {}\n",
    "            \n",
    "            # Extract title and URL\n",
    "            title_elem = news_element.find('h5', class_='event-title')\n",
    "            if title_elem and title_elem.find('a'):\n",
    "                news['title'] = title_elem.find('a').text.strip()\n",
    "                news['article_url'] = self.base_url + title_elem.find('a')['href']\n",
    "            \n",
    "            # Extract image URL\n",
    "            img_elem = news_element.find('img', class_='img-responsive')\n",
    "            if img_elem:\n",
    "                img_src = img_elem.get('src', '')\n",
    "                news['image_url'] = self.base_url + img_src if not img_src.startswith('http') else img_src\n",
    "\n",
    "            # Extract date if available\n",
    "            date_elem = news_element.find('span', class_='date-display-single')\n",
    "            if date_elem:\n",
    "                news['date'] = date_elem.text.strip()\n",
    "\n",
    "            # Get full article details\n",
    "            if news.get('article_url'):\n",
    "                article_details = self.get_article_details(news['article_url'])\n",
    "                news.update(article_details)\n",
    "\n",
    "            return news\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting news item: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_article_details(self, article_url):\n",
    "        \"\"\"Scrape detailed information from the article page\"\"\"\n",
    "        print(f\"Fetching details from: {article_url}\")\n",
    "        soup = self.get_soup(article_url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "        \n",
    "        details = {}\n",
    "        try:\n",
    "            # Extract article content from node__content\n",
    "            content_div = soup.find('div', class_='node__content')\n",
    "            if content_div:\n",
    "                # Get all paragraphs and their text\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                \n",
    "                # Check if first paragraph contains location/date\n",
    "                if paragraphs and ' - ' in paragraphs[0].text:\n",
    "                    first_p = paragraphs[0].text.strip()\n",
    "                    if len(first_p) < 100:  # Location/date line is typically short\n",
    "                        details['location_date'] = first_p\n",
    "                        # Remove first paragraph from content if it's just location/date\n",
    "                        paragraphs = paragraphs[1:]\n",
    "                \n",
    "                # Join remaining paragraphs for full content\n",
    "                content = '\\n\\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "                details['description'] = content\n",
    "                \n",
    "                # Try to extract date if available\n",
    "                date_elem = soup.find('span', class_='date-display-single')\n",
    "                if date_elem:\n",
    "                    details['article_date'] = date_elem.text.strip()\n",
    "                \n",
    "                # Try to get author if available\n",
    "                author_elem = soup.find('div', class_='field--name-field-author')\n",
    "                if author_elem:\n",
    "                    details['author'] = author_elem.text.strip()\n",
    "                \n",
    "                # Get any tags/categories\n",
    "                tags_container = soup.find('div', class_='field--name-field-tags')\n",
    "                if tags_container:\n",
    "                    tags = tags_container.find_all('a')\n",
    "                    details['tags'] = [tag.text.strip() for tag in tags]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article details from {article_url}: {str(e)}\")\n",
    "        \n",
    "        return details\n",
    "\n",
    "    def check_load_more_button(self, soup):\n",
    "        \"\"\"Check if there's a 'Load More' button on the page\"\"\"\n",
    "        pager = soup.find('ul', class_='js-pager__items')\n",
    "        if pager:\n",
    "            load_more = pager.find('a', rel='next')\n",
    "            return bool(load_more)\n",
    "        return False\n",
    "\n",
    "    def get_next_page_url(self, soup):\n",
    "        \"\"\"Extract the next page URL from the Load More button\"\"\"\n",
    "        pager = soup.find('ul', class_='js-pager__items')\n",
    "        if pager:\n",
    "            load_more = pager.find('a', rel='next')\n",
    "            if load_more and 'href' in load_more.attrs:\n",
    "                return load_more['href']\n",
    "        return None\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Scrape a single page of news items\"\"\"\n",
    "        print(f\"\\nScraping page: {url}\")\n",
    "        \n",
    "        soup = self.get_soup(url)\n",
    "        if not soup:\n",
    "            return False, None\n",
    "\n",
    "        # Handle both top stories and regular news\n",
    "        news_containers = soup.find_all('div', class_=['event-card top-story', 'event-card more-news'])\n",
    "        \n",
    "        for container in news_containers:\n",
    "            news_info = self.extract_news_item(container)\n",
    "            if news_info:\n",
    "                self.news_data.append(news_info)\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "\n",
    "        # Check for next page\n",
    "        has_more = self.check_load_more_button(soup)\n",
    "        next_url = self.get_next_page_url(soup) if has_more else None\n",
    "        \n",
    "        return has_more, next_url\n",
    "\n",
    "    def scrape_all_news(self, max_pages=None):\n",
    "        \"\"\"Scrape all news pages up to max_pages\"\"\"\n",
    "        current_url = f\"{self.base_url}/en/news\"\n",
    "        page_count = 0\n",
    "        \n",
    "        while current_url:\n",
    "            if max_pages is not None and page_count >= max_pages:\n",
    "                break\n",
    "                \n",
    "            has_more, next_url = self.scrape_page(current_url)\n",
    "            \n",
    "            if not has_more or not next_url:\n",
    "                break\n",
    "                \n",
    "            if next_url.startswith('?'):\n",
    "                current_url = f\"{self.base_url}/en/news{next_url}\"\n",
    "            else:\n",
    "                current_url = f\"{self.base_url}{next_url}\"\n",
    "                \n",
    "            page_count += 1\n",
    "            time.sleep(2)  \n",
    "        \n",
    "        print(f\"\\nCompleted scraping {page_count + 1} pages\")\n",
    "        print(f\"Total articles collected: {len(self.news_data)}\")\n",
    "\n",
    "    def save_to_json(self, filename='news_data.json'):\n",
    "        \"\"\"Save scraped data to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'scrape_date': datetime.now().isoformat(),\n",
    "                    'total_articles': len(self.news_data),\n",
    "                    'articles': self.news_data\n",
    "                }, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nSuccessfully saved {len(self.news_data)} articles to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to JSON: {str(e)}\")\n",
    "\n",
    "    def save_to_mongo(self):\n",
    "        \"\"\"Save scraped data to MongoDB\"\"\"\n",
    "        try:\n",
    "            if self.news_data:\n",
    "                news_collection.insert_many(self.news_data)\n",
    "                print(f\"\\nSuccessfully inserted {len(self.news_data)} articles into MongoDB\")\n",
    "            else:\n",
    "                print(\"No data to save to MongoDB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to MongoDB: {str(e)}\")\n",
    "\n",
    "\n",
    "@app.route('/news/scrape', methods=['GET'])\n",
    "def scrape_news():\n",
    "    base_url = \"https://institute.aljazeera.net\"\n",
    "    scraper = NewsScraper(base_url)\n",
    "    \n",
    "    try:\n",
    "        # max_pages = request.args.get('max_pages', default=4, type=int)\n",
    "        scraper.scrape_all_news(max_pages=5)\n",
    "        scraper.save_to_mongo()  # Save data to MongoDB instead of JSON\n",
    "        return jsonify({\"message\": \"News scraping completed successfully\", \"articles_collected\": len(scraper.news_data)}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "@app.route('/news', methods=['GET'])\n",
    "def get_news():\n",
    "    try:\n",
    "        news = list(news_collection.find({}, {'_id': 0}))  # Exclude MongoDB's default _id field\n",
    "        return jsonify(news), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "\n",
    "### Course Scraper ###\n",
    "def scrape_course_details(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        details = {\n",
    "            'Title': clean_text(soup.select_one('h2.course-subtitle').get_text()),\n",
    "            'Trainer': clean_text(soup.select_one('.course-details-s1 p + a').get_text()),\n",
    "            'Time': clean_text(soup.select('.course-details-s1 p')[3].get_text().replace('Category:', '').strip()),\n",
    "            'Price': clean_text(soup.select_one('.price .discount-data-bk1').get_text()),\n",
    "            'Prerequisites': clean_text(soup.find('p', class_='sub-heading', string='Prerequisites').find_next('p').get_text()),\n",
    "            'Description': clean_text(soup.find('p', class_='sub-heading', string='Course Description').find_next('p').get_text()),\n",
    "            'Objectives': clean_text(soup.find('p', class_='sub-heading', string='Course Objective').find_next('p').get_text()),\n",
    "            'Outline': clean_text(soup.find('p', class_='sub-heading', string='Course Outline').find_next('p').get_text()),\n",
    "            'Benefits': clean_text(soup.find('p', class_='sub-heading', string='Course Benefits').find_next('p').get_text())\n",
    "        }\n",
    "        return details\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping course details from {url}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def scrape_all_courses(base_url):\n",
    "    courses = []\n",
    "    page = 0\n",
    "\n",
    "    while True:\n",
    "        page_url = f\"{base_url}?page={page}\"\n",
    "        print(f\"Scraping courses page: {page_url}\")\n",
    "        \n",
    "        response = requests.get(page_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        course_cards = soup.select('#course-filter-results .course-card')\n",
    "        if not course_cards:\n",
    "            print(\"No more course cards found. Stopping.\")\n",
    "            break\n",
    "\n",
    "        for card in course_cards:\n",
    "            link = card.find('a')['href']\n",
    "            course_url = 'https://institute.aljazeera.net' + link\n",
    "            print(f\"Scraping course: {course_url}\")\n",
    "\n",
    "            course_info = {\n",
    "                'Title': clean_text(card.select_one('.course-title').text),\n",
    "                'Date and Time': clean_text(card.select_one('.course-date').text),\n",
    "                'Description': clean_text(card.select_one('.card-desc').text),\n",
    "                'Image URL': card.select_one('.course-img-top')['src'],\n",
    "                'Course URL': course_url\n",
    "            }\n",
    "\n",
    "            detailed_info = scrape_course_details(course_url)\n",
    "            if detailed_info:\n",
    "                course_info.update(detailed_info)\n",
    "\n",
    "            courses.append(course_info)\n",
    "            time.sleep(1)\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return courses\n",
    "\n",
    "### Trainer Scraper ###\n",
    "class TrainerScraper:\n",
    "    def __init__(self, base_url=\"https://institute.aljazeera.net\"):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.trainers = []\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        response = self.session.get(url)\n",
    "        if response.status_code == 200:\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        return None\n",
    "\n",
    "    def get_total_pages(self, soup):\n",
    "        pagination = soup.find('ul', class_='pagination')\n",
    "        if pagination:\n",
    "            links = pagination.find_all('li')\n",
    "            page_no = []\n",
    "            for link in links:\n",
    "                if link.find('a'):\n",
    "                    try:\n",
    "                        num = int(link.find('a').text.strip())\n",
    "                        page_no.append(num)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            if page_no:\n",
    "                return max(page_no)\n",
    "\n",
    "        return 3  # Default to 3 pages since we know they exist\n",
    "\n",
    "    def extract_trainer_info(self, trainer_box):\n",
    "        trainer = {}\n",
    "        name_elem = trainer_box.find('h4', class_='header')\n",
    "        trainer['name'] = name_elem.text.strip() if name_elem else None\n",
    "        \n",
    "        org_elem = trainer_box.find('h5', class_='header')\n",
    "        trainer['organization'] = org_elem.text.strip() if org_elem else None\n",
    "        \n",
    "        link_elem = trainer_box.find('a')\n",
    "        if link_elem and link_elem.get('href'):\n",
    "            trainer['profile_url'] = self.base_url + link_elem.get('href')\n",
    "        \n",
    "        img_elem = trainer_box.find('img')\n",
    "        if img_elem:\n",
    "            img_src = img_elem.get('src')\n",
    "            trainer['image_url'] = img_src if img_src.startswith('http') else self.base_url + img_src\n",
    "\n",
    "        return trainer\n",
    "\n",
    "    def get_trainer_details(self, profile_url):\n",
    "        soup = self.get_soup(profile_url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "        \n",
    "        details = {}\n",
    "        trainer_details = soup.find('div', id='trainer-details')\n",
    "        if trainer_details:\n",
    "            trainer_info = trainer_details.find('div', class_='trainer-info')\n",
    "            if trainer_info:\n",
    "                paragraphs = trainer_info.find_all('p')\n",
    "                if len(paragraphs) > 1:\n",
    "                    details['specialization'] = paragraphs[-1].text.strip()\n",
    "            \n",
    "            bio_heading = trainer_details.find('p', class_='sub-heading', string=re.compile(r'Bio', re.IGNORECASE))\n",
    "            if bio_heading:\n",
    "                bio = bio_heading.find_next('p')\n",
    "                if bio:\n",
    "                    details['biography'] = bio.text.strip()\n",
    "\n",
    "            exp_heading = trainer_details.find('p', class_='sub-heading', string=re.compile(r'Experience', re.IGNORECASE))\n",
    "            if exp_heading:\n",
    "                exp = exp_heading.find_next('p')\n",
    "                if exp:\n",
    "                    exp_text = exp.text.strip()\n",
    "                    exp_points = [point.strip() for point in re.split(r'\\d+\\.', exp_text) if point.strip()]\n",
    "                    details['experience'] = exp_points\n",
    "\n",
    "            edu_heading = trainer_details.find('p', class_='sub-heading', string=re.compile(r'Education', re.IGNORECASE))\n",
    "            if edu_heading:\n",
    "                edu = edu_heading.find_next('p')\n",
    "                if edu:\n",
    "                    details['education'] = edu.text.strip()\n",
    "\n",
    "        return details\n",
    "\n",
    "    def scrape_trainers(self, max_pages=None):\n",
    "        first_page_url = f\"{self.base_url}/en/trainers\"\n",
    "        first_page_soup = self.get_soup(first_page_url)\n",
    "        \n",
    "        if not first_page_soup:\n",
    "            print(\"Failed to access the first page\")\n",
    "            return\n",
    "        \n",
    "        total_pages = self.get_total_pages(first_page_soup)\n",
    "        if max_pages:\n",
    "            total_pages = min(total_pages, max_pages)\n",
    "        \n",
    "        print(f\"Found {total_pages} pages to scrape\")\n",
    "        \n",
    "        for page in range(total_pages):\n",
    "            page_num = page + 1\n",
    "            url = f\"{self.base_url}/en/trainers?page={page_num}\"\n",
    "            print(f\"Scraping page {page_num} of {total_pages}\")\n",
    "            \n",
    "            soup = self.get_soup(url)\n",
    "            if not soup:\n",
    "                print(f\"Failed to access page {page_num}\")\n",
    "                continue\n",
    "            \n",
    "            trainer_boxes = soup.find_all('div', class_='trainer-box')\n",
    "            \n",
    "            for box in trainer_boxes:\n",
    "                trainer_info = self.extract_trainer_info(box)\n",
    "                \n",
    "                if trainer_info.get('profile_url'):\n",
    "                    print(f\"Scraping details for {trainer_info['name']}\")\n",
    "                    details = self.get_trainer_details(trainer_info['profile_url'])\n",
    "                    trainer_info.update(details)\n",
    "                \n",
    "                self.trainers.append(trainer_info)\n",
    "                time.sleep(1)  # Rate limiting\n",
    "            \n",
    "            print(f\"Completed page {page_num}\")\n",
    "\n",
    "    def save_to_mongo(self):\n",
    "        trainers_collection.delete_many({})  # Clear the collection before saving new data\n",
    "        trainers_collection.insert_many(self.trainers)\n",
    "        print(f\"Saved {len(self.trainers)} trainers to MongoDB.\")\n",
    "\n",
    "### Flask Endpoints ###\n",
    "@app.route('/refresh', methods=['POST'])\n",
    "def refresh_data():\n",
    "    # Scrape and save data\n",
    "    print(\"Starting refresh process...\")\n",
    "    \n",
    "    # Clear previous collections\n",
    "    news_collection.delete_many({})\n",
    "    courses_collection.delete_many({})\n",
    "    trainers_collection.delete_many({})\n",
    "\n",
    "    # # Scrape news\n",
    "    # news_data = scrape_news(\"https://institute.aljazeera.net/en/news\")\n",
    "    # news_collection.insert_many(news_data)\n",
    "\n",
    "    # Scrape courses\n",
    "    courses_data = scrape_all_courses(\"https://institute.aljazeera.net/en/courses\")\n",
    "    courses_collection.insert_many(courses_data)\n",
    "\n",
    "    # Scrape trainers\n",
    "    trainer_scraper = TrainerScraper()\n",
    "    trainer_scraper.scrape_trainers(max_pages=3)\n",
    "    trainer_scraper.save_to_mongo()\n",
    "\n",
    "    return jsonify({'message': 'Data refreshed successfully'}), 200\n",
    "\n",
    "# @app.route('/news', methods=['GET'])\n",
    "def fetch_news():\n",
    "    news = list(news_collection.find({}, {'_id': 0}))\n",
    "    return jsonify(news)\n",
    "\n",
    "@app.route('/courses', methods=['GET'])\n",
    "def get_courses():\n",
    "    courses = list(courses_collection.find({}, {'_id': 0}))\n",
    "    return jsonify(courses)\n",
    "\n",
    "@app.route('/trainers', methods=['GET'])\n",
    "def get_trainers():\n",
    "    trainers = list(trainers_collection.find({}, {'_id': 0}))\n",
    "    return jsonify(trainers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping courses page: https://institute.aljazeera.net/en/courses?page=0\n",
      "Scraping course: https://institute.aljazeera.net/en/course/12710\n",
      "Scraping course: https://institute.aljazeera.net/en/course/10995\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrape_all_courses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://institute.aljazeera.net/en/courses\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 296\u001b[0m, in \u001b[0;36mscrape_all_courses\u001b[1;34m(base_url)\u001b[0m\n\u001b[0;32m    293\u001b[0m             course_info\u001b[38;5;241m.\u001b[39mupdate(detailed_info)\n\u001b[0;32m    295\u001b[0m         courses\u001b[38;5;241m.\u001b[39mappend(course_info)\n\u001b[1;32m--> 296\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     page \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m courses\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape_all_courses(\"https://institute.aljazeera.net/en/courses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.news_data = []\n",
    "        \n",
    "        # Set up session headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Get BeautifulSoup object for a given URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_news_item(self, news_element):\n",
    "        \"\"\"Extract information from a news item element\"\"\"\n",
    "        try:\n",
    "            news = {}\n",
    "            \n",
    "            # Extract title and URL\n",
    "            title_elem = news_element.find('h5', class_='event-title')\n",
    "            if title_elem and title_elem.find('a'):\n",
    "                news['title'] = title_elem.find('a').text.strip()\n",
    "                news['article_url'] = self.base_url + title_elem.find('a')['href']\n",
    "            \n",
    "            # Extract image URL\n",
    "            img_elem = news_element.find('img', class_='img-responsive')\n",
    "            if img_elem:\n",
    "                img_src = img_elem.get('src', '')\n",
    "                news['image_url'] = self.base_url + img_src if not img_src.startswith('http') else img_src\n",
    "\n",
    "            # Extract date if available\n",
    "            date_elem = news_element.find('span', class_='date-display-single')\n",
    "            if date_elem:\n",
    "                news['date'] = date_elem.text.strip()\n",
    "\n",
    "            # Get full article details\n",
    "            if news.get('article_url'):\n",
    "                article_details = self.get_article_details(news['article_url'])\n",
    "                news.update(article_details)\n",
    "\n",
    "            return news\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting news item: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_article_details(self, article_url):\n",
    "        \"\"\"Scrape detailed information from the article page\"\"\"\n",
    "        print(f\"Fetching details from: {article_url}\")\n",
    "        soup = self.get_soup(article_url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "        \n",
    "        details = {}\n",
    "        try:\n",
    "            # Extract article content from node__content\n",
    "            content_div = soup.find('div', class_='node__content')\n",
    "            if content_div:\n",
    "                # Get all paragraphs and their text\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                \n",
    "                # Check if first paragraph contains location/date\n",
    "                if paragraphs and ' - ' in paragraphs[0].text:\n",
    "                    first_p = paragraphs[0].text.strip()\n",
    "                    if len(first_p) < 100:  # Location/date line is typically short\n",
    "                        details['location_date'] = first_p\n",
    "                        # Remove first paragraph from content if it's just location/date\n",
    "                        paragraphs = paragraphs[1:]\n",
    "                \n",
    "                # Join remaining paragraphs for full content\n",
    "                content = '\\n\\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "                details['description'] = content\n",
    "                \n",
    "                # Try to extract date if available\n",
    "                date_elem = soup.find('span', class_='date-display-single')\n",
    "                if date_elem:\n",
    "                    details['article_date'] = date_elem.text.strip()\n",
    "                \n",
    "                # Try to get author if available\n",
    "                author_elem = soup.find('div', class_='field--name-field-author')\n",
    "                if author_elem:\n",
    "                    details['author'] = author_elem.text.strip()\n",
    "                \n",
    "                # Get any tags/categories\n",
    "                tags_container = soup.find('div', class_='field--name-field-tags')\n",
    "                if tags_container:\n",
    "                    tags = tags_container.find_all('a')\n",
    "                    details['tags'] = [tag.text.strip() for tag in tags]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article details from {article_url}: {str(e)}\")\n",
    "        \n",
    "        return details\n",
    "\n",
    "    def check_load_more_button(self, soup):\n",
    "        \"\"\"Check if there's a 'Load More' button on the page\"\"\"\n",
    "        pager = soup.find('ul', class_='js-pager__items')\n",
    "        if pager:\n",
    "            load_more = pager.find('a', rel='next')\n",
    "            return bool(load_more)\n",
    "        return False\n",
    "\n",
    "    def get_next_page_url(self, soup):\n",
    "        \"\"\"Extract the next page URL from the Load More button\"\"\"\n",
    "        pager = soup.find('ul', class_='js-pager__items')\n",
    "        if pager:\n",
    "            load_more = pager.find('a', rel='next')\n",
    "            if load_more and 'href' in load_more.attrs:\n",
    "                return load_more['href']\n",
    "        return None\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Scrape a single page of news items\"\"\"\n",
    "        print(f\"\\nScraping page: {url}\")\n",
    "        \n",
    "        soup = self.get_soup(url)\n",
    "        if not soup:\n",
    "            return False, None\n",
    "\n",
    "        # Handle both top stories and regular news\n",
    "        news_containers = soup.find_all('div', class_=['event-card top-story', 'event-card more-news'])\n",
    "        \n",
    "        for container in news_containers:\n",
    "            news_info = self.extract_news_item(container)\n",
    "            if news_info:\n",
    "                self.news_data.append(news_info)\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "\n",
    "        # Check for next page\n",
    "        has_more = self.check_load_more_button(soup)\n",
    "        next_url = self.get_next_page_url(soup) if has_more else None\n",
    "        \n",
    "        return has_more, next_url\n",
    "\n",
    "    def scrape_all_news(self, max_pages=None):\n",
    "        \"\"\"Scrape all news pages up to max_pages\"\"\"\n",
    "        current_url = f\"{self.base_url}/en/news\"\n",
    "        page_count = 0\n",
    "        \n",
    "        while current_url:\n",
    "            if max_pages is not None and page_count >= max_pages:\n",
    "                break\n",
    "                \n",
    "            has_more, next_url = self.scrape_page(current_url)\n",
    "            \n",
    "            if not has_more or not next_url:\n",
    "                break\n",
    "                \n",
    "            if next_url.startswith('?'):\n",
    "                current_url = f\"{self.base_url}/en/news{next_url}\"\n",
    "            else:\n",
    "                current_url = f\"{self.base_url}{next_url}\"\n",
    "                \n",
    "            page_count += 1\n",
    "            time.sleep(2)  \n",
    "        \n",
    "        print(f\"\\nCompleted scraping {page_count + 1} pages\")\n",
    "        print(f\"Total articles collected: {len(self.news_data)}\")\n",
    "\n",
    "    def save_to_json(self, filename='news_data.json'):\n",
    "        \"\"\"Save scraped data to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'scrape_date': datetime.now().isoformat(),\n",
    "                    'total_articles': len(self.news_data),\n",
    "                    'articles': self.news_data\n",
    "                }, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nSuccessfully saved {len(self.news_data)} articles to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to JSON: {str(e)}\")\n",
    "\n",
    "    def save_to_mongo(self):\n",
    "        \"\"\"Save scraped data to MongoDB\"\"\"\n",
    "        try:\n",
    "            if self.news_data:\n",
    "                news_collection.insert_many(self.news_data)\n",
    "                print(f\"\\nSuccessfully inserted {len(self.news_data)} articles into MongoDB\")\n",
    "            else:\n",
    "                print(\"No data to save to MongoDB\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to MongoDB: {str(e)}\")\n",
    "\n",
    "\n",
    "# @app.route('/news/scrape', methods=['GET'])\n",
    "def scrape_news():\n",
    "    base_url = \"https://institute.aljazeera.net\"\n",
    "    scraper = NewsScraper(base_url)\n",
    "    \n",
    "    try:\n",
    "        # max_pages = request.args.get('max_pages', default=4, type=int)\n",
    "        scraper.scrape_all_news(max_pages=5)\n",
    "        scraper.save_to_mongo()  # Save data to MongoDB instead of JSON\n",
    "        return jsonify({\"message\": \"News scraping completed successfully\", \"articles_collected\": len(scraper.news_data)}), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "\n",
    "# @app.route('/news', methods=['GET'])\n",
    "def get_news():\n",
    "    try:\n",
    "        news = list(news_collection.find({}, {'_id': 0}))  # Exclude MongoDB's default _id field\n",
    "        return jsonify(news), 200\n",
    "    except Exception as e:\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "    \n",
    "# base_url = \"https://institute.aljazeera.net\"\n",
    "# scraper = NewsScraper(base_url)\n",
    "\n",
    "# try:\n",
    "#     scraper.scrape_all_news(max_pages=4)  \n",
    "#     scraper.save_to_json()\n",
    "#     print(\"News scraping completed successfully\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred during scraping: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping page: https://institute.aljazeera.net/en/news\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/e-learning-platform-launches-%E2%80%9Cwar-reporting%E2%80%9D-course\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/al-jazeera-institute-concludes-%E2%80%9Cdevelop-english-writing-program%E2%80%9D\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/issuance-official-statements-and-declarations-audit-guide-ajmi\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/al-jazeera-media-institute-wins-two-stevie-awards\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mscrape_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 199\u001b[0m, in \u001b[0;36mscrape_news\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m scraper \u001b[38;5;241m=\u001b[39m NewsScraper(base_url)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;66;03m# max_pages = request.args.get('max_pages', default=4, type=int)\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_all_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     scraper\u001b[38;5;241m.\u001b[39msave_to_mongo()  \u001b[38;5;66;03m# Save data to MongoDB instead of JSON\u001b[39;00m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jsonify({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews scraping completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticles_collected\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(scraper\u001b[38;5;241m.\u001b[39mnews_data)}), \u001b[38;5;241m200\u001b[39m\n",
      "Cell \u001b[1;32mIn[10], line 151\u001b[0m, in \u001b[0;36mNewsScraper.scrape_all_news\u001b[1;34m(self, max_pages)\u001b[0m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_pages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m page_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_pages:\n\u001b[0;32m    149\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m has_more, next_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_more \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m next_url:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 131\u001b[0m, in \u001b[0;36mNewsScraper.scrape_page\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    128\u001b[0m news_containers \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent-card top-story\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent-card more-news\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m container \u001b[38;5;129;01min\u001b[39;00m news_containers:\n\u001b[1;32m--> 131\u001b[0m     news_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_news_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m news_info:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_data\u001b[38;5;241m.\u001b[39mappend(news_info)\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36mNewsScraper.extract_news_item\u001b[1;34m(self, news_element)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Get full article details\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m news\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle_url\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m---> 46\u001b[0m     article_details \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_article_details\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43marticle_url\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m     news\u001b[38;5;241m.\u001b[39mupdate(article_details)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m news\n",
      "Cell \u001b[1;32mIn[10], line 57\u001b[0m, in \u001b[0;36mNewsScraper.get_article_details\u001b[1;34m(self, article_url)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Scrape detailed information from the article page\"\"\"\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching details from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marticle_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_soup\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m soup:\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m, in \u001b[0;36mNewsScraper.get_soup\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Get BeautifulSoup object for a given URL\"\"\"\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\requests\\sessions.py:602\u001b[0m, in \u001b[0;36mSession.get\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[0;32m    595\u001b[0m \n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m:rtype: requests.Response\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    601\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\requests\\sessions.py:589\u001b[0m, in \u001b[0;36mSession.request\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    584\u001b[0m send_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m\"\u001b[39m: timeout,\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m: allow_redirects,\n\u001b[0;32m    587\u001b[0m }\n\u001b[0;32m    588\u001b[0m send_kwargs\u001b[38;5;241m.\u001b[39mupdate(settings)\n\u001b[1;32m--> 589\u001b[0m resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\requests\\sessions.py:703\u001b[0m, in \u001b[0;36mSession.send\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    700\u001b[0m start \u001b[38;5;241m=\u001b[39m preferred_clock()\n\u001b[0;32m    702\u001b[0m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43madapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[0;32m    706\u001b[0m elapsed \u001b[38;5;241m=\u001b[39m preferred_clock() \u001b[38;5;241m-\u001b[39m start\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\requests\\adapters.py:667\u001b[0m, in \u001b[0;36mHTTPAdapter.send\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    664\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m TimeoutSauce(connect\u001b[38;5;241m=\u001b[39mtimeout, read\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 667\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    677\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    682\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request\u001b[38;5;241m=\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\urllib3\\connectionpool.py:789\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    786\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    805\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\Mrinal Bhan\\Cerebrum\\Az-Scraper\\az-env\\Lib\\site-packages\\urllib3\\connection.py:507\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\http\\client.py:1428\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1426\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1427\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1428\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1429\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1430\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\http\\client.py:331\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 331\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\http\\client.py:292\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 292\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\socket.py:720\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 720\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    722\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\ssl.py:1251\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1249\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1250\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1252\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Python312\\Lib\\ssl.py:1103\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1101\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1103\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1104\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1105\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scraping page: https://institute.aljazeera.net/en/news\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/e-learning-platform-launches-%E2%80%9Cwar-reporting%E2%80%9D-course\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/al-jazeera-institute-concludes-%E2%80%9Cdevelop-english-writing-program%E2%80%9D\n",
      "Fetching details from: https://institute.aljazeera.net/en/news/issuance-official-statements-and-declarations-audit-guide-ajmi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 191\u001b[0m\n\u001b[0;32m    188\u001b[0m scraper \u001b[38;5;241m=\u001b[39m NewsScraper(base_url)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[43mscraper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_all_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_pages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[0;32m    192\u001b[0m     scraper\u001b[38;5;241m.\u001b[39msave_to_json()\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNews scraping completed successfully\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[6], line 157\u001b[0m, in \u001b[0;36mNewsScraper.scrape_all_news\u001b[1;34m(self, max_pages)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_pages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m page_count \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m max_pages:\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m has_more, next_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscrape_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_more \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m next_url:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 140\u001b[0m, in \u001b[0;36mNewsScraper.scrape_page\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m news_info:\n\u001b[0;32m    139\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnews_data\u001b[38;5;241m.\u001b[39mappend(news_info)\n\u001b[1;32m--> 140\u001b[0m         \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Polite delay between requests\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;66;03m# Check for next page\u001b[39;00m\n\u001b[0;32m    143\u001b[0m has_more \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_load_more_button(soup)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "class NewsScraper:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.session = requests.Session()\n",
    "        self.news_data = []\n",
    "        \n",
    "        # Set up session headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "\n",
    "    def get_soup(self, url):\n",
    "        \"\"\"Get BeautifulSoup object for a given URL\"\"\"\n",
    "        try:\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.text, 'html.parser')\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def extract_news_item(self, news_element):\n",
    "        \"\"\"Extract information from a news item element\"\"\"\n",
    "        try:\n",
    "            news = {}\n",
    "            \n",
    "            # Extract title and URL\n",
    "            title_elem = news_element.find('h5', class_='event-title')\n",
    "            if title_elem and title_elem.find('a'):\n",
    "                news['title'] = title_elem.find('a').text.strip()\n",
    "                news['article_url'] = self.base_url + title_elem.find('a')['href']\n",
    "            \n",
    "            # Extract image URL\n",
    "            img_elem = news_element.find('img', class_='img-responsive')\n",
    "            if img_elem:\n",
    "                img_src = img_elem.get('src', '')\n",
    "                news['image_url'] = self.base_url + img_src if not img_src.startswith('http') else img_src\n",
    "\n",
    "            # Extract date if available\n",
    "            date_elem = news_element.find('span', class_='date-display-single')\n",
    "            if date_elem:\n",
    "                news['date'] = date_elem.text.strip()\n",
    "\n",
    "            # Get full article details\n",
    "            if news.get('article_url'):\n",
    "                article_details = self.get_article_details(news['article_url'])\n",
    "                news.update(article_details)\n",
    "\n",
    "            return news\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting news item: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def get_article_details(self, article_url):\n",
    "        \"\"\"Scrape detailed information from the article page\"\"\"\n",
    "        print(f\"Fetching details from: {article_url}\")\n",
    "        soup = self.get_soup(article_url)\n",
    "        if not soup:\n",
    "            return {}\n",
    "        \n",
    "        details = {}\n",
    "        try:\n",
    "            # Extract article content from node__content\n",
    "            content_div = soup.find('div', class_='node__content')\n",
    "            if content_div:\n",
    "                # Get all paragraphs and their text\n",
    "                paragraphs = content_div.find_all('p')\n",
    "                \n",
    "                # Check if first paragraph contains location/date\n",
    "                if paragraphs and ' - ' in paragraphs[0].text:\n",
    "                    first_p = paragraphs[0].text.strip()\n",
    "                    if len(first_p) < 100:  # Location/date line is typically short\n",
    "                        details['location_date'] = first_p\n",
    "                        # Remove first paragraph from content if it's just location/date\n",
    "                        paragraphs = paragraphs[1:]\n",
    "                \n",
    "                # Join remaining paragraphs for full content\n",
    "                content = '\\n\\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])\n",
    "                details['description'] = content\n",
    "                \n",
    "                # Try to extract date if available\n",
    "                date_elem = soup.find('span', class_='date-display-single')\n",
    "                if date_elem:\n",
    "                    details['article_date'] = date_elem.text.strip()\n",
    "                \n",
    "                # Try to get author if available\n",
    "                author_elem = soup.find('div', class_='field--name-field-author')\n",
    "                if author_elem:\n",
    "                    details['author'] = author_elem.text.strip()\n",
    "                \n",
    "                # Get any tags/categories\n",
    "                tags_container = soup.find('div', class_='field--name-field-tags')\n",
    "                if tags_container:\n",
    "                    tags = tags_container.find_all('a')\n",
    "                    details['tags'] = [tag.text.strip() for tag in tags]\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting article details from {article_url}: {str(e)}\")\n",
    "        \n",
    "        return details\n",
    "\n",
    "    def check_load_more_button(self, soup):\n",
    "        \"\"\"Check if there's a 'Load More' button on the page\"\"\"\n",
    "        pager = soup.find('ul', class_='js-pager__items')\n",
    "        if pager:\n",
    "            load_more = pager.find('a', rel='next')\n",
    "            return bool(load_more)\n",
    "        return False\n",
    "\n",
    "    def get_next_page_url(self, soup):\n",
    "        \"\"\"Extract the next page URL from the Load More button\"\"\"\n",
    "        pager = soup.find('ul', class_='js-pager__items')\n",
    "        if pager:\n",
    "            load_more = pager.find('a', rel='next')\n",
    "            if load_more and 'href' in load_more.attrs:\n",
    "                return load_more['href']\n",
    "        return None\n",
    "\n",
    "    def scrape_page(self, url):\n",
    "        \"\"\"Scrape a single page of news items\"\"\"\n",
    "        print(f\"\\nScraping page: {url}\")\n",
    "        \n",
    "        soup = self.get_soup(url)\n",
    "        if not soup:\n",
    "            return False, None\n",
    "\n",
    "        # Handle both top stories and regular news\n",
    "        news_containers = soup.find_all('div', class_=['event-card top-story', 'event-card more-news'])\n",
    "        \n",
    "        for container in news_containers:\n",
    "            news_info = self.extract_news_item(container)\n",
    "            if news_info:\n",
    "                self.news_data.append(news_info)\n",
    "                time.sleep(1)  # Polite delay between requests\n",
    "\n",
    "        # Check for next page\n",
    "        has_more = self.check_load_more_button(soup)\n",
    "        next_url = self.get_next_page_url(soup) if has_more else None\n",
    "        \n",
    "        return has_more, next_url\n",
    "\n",
    "    def scrape_all_news(self, max_pages=None):\n",
    "        \"\"\"Scrape all news pages up to max_pages\"\"\"\n",
    "        current_url = f\"{self.base_url}/en/news\"\n",
    "        page_count = 0\n",
    "        \n",
    "        while current_url:\n",
    "            if max_pages is not None and page_count >= max_pages:\n",
    "                break\n",
    "                \n",
    "            has_more, next_url = self.scrape_page(current_url)\n",
    "            \n",
    "            if not has_more or not next_url:\n",
    "                break\n",
    "                \n",
    "            if next_url.startswith('?'):\n",
    "                current_url = f\"{self.base_url}/en/news{next_url}\"\n",
    "            else:\n",
    "                current_url = f\"{self.base_url}{next_url}\"\n",
    "                \n",
    "            page_count += 1\n",
    "            time.sleep(2)  \n",
    "        \n",
    "        print(f\"\\nCompleted scraping {page_count + 1} pages\")\n",
    "        print(f\"Total articles collected: {len(self.news_data)}\")\n",
    "\n",
    "    def save_to_json(self, filename='news_data.json'):\n",
    "        \"\"\"Save scraped data to JSON file\"\"\"\n",
    "        try:\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump({\n",
    "                    'scrape_date': datetime.now().isoformat(),\n",
    "                    'total_articles': len(self.news_data),\n",
    "                    'articles': self.news_data\n",
    "                }, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"\\nSuccessfully saved {len(self.news_data)} articles to {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to JSON: {str(e)}\")\n",
    "\n",
    "\n",
    "base_url = \"https://institute.aljazeera.net\"\n",
    "scraper = NewsScraper(base_url)\n",
    "\n",
    "try:\n",
    "    scraper.scrape_all_news(max_pages=4)  \n",
    "    scraper.save_to_json()\n",
    "    print(\"News scraping completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during scraping: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "az-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
